import librosa
import torch
import torch.nn as nn
import torchaudio
import torch.nn.functional as F
import numpy as np
import pyaudio
import os


# --- T·∫°o vocab: ch·ªØ c√°i v√† k√Ω t·ª± c∆° b·∫£n ---
vocab = {ch: idx for idx, ch in enumerate("abcdefghijklmnopqrstuvwxyz ',.!?", start=1)}
vocab['<unk>'] = 0

# --- H√†m x·ª≠ l√Ω vƒÉn b·∫£n ---
def text_to_sequence(text, vocab):
    return [vocab.get(ch, vocab['<unk>']) for ch in text.lower() if ch in vocab]

# --- H√†m x·ª≠ l√Ω √¢m thanh ---
def audio_to_mel(audio_path):
    waveform, sample_rate = torchaudio.load(audio_path)
    transform = torchaudio.transforms.MelSpectrogram(
        sample_rate=sample_rate, 
        n_mels=80
    )
    mel = transform(waveform)  # [1, mel_dim, time_steps]
    return mel

# --- M√¥ h√¨nh Speech Synthesis ƒë∆°n gi·∫£n v·ªõi vocoder CNN ---
class SpeechSynthesizer(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, mel_dim=80):
        super(SpeechSynthesizer, self).__init__()
        # Text to embedding
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # Encoder: LSTM layer
        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        
        # Decoder: Linear to output mel-spectrogram
        self.linear = nn.Linear(hidden_dim, mel_dim)

    def forward(self, text_seq):
        embedded = self.embedding(text_seq)  # [batch, seq_len, embedding_dim]
        encoder_output, _ = self.encoder(embedded)  # [batch, seq_len, hidden_dim]
        mel_output = self.linear(encoder_output)  # [batch, seq_len, mel_dim]
        return mel_output


class VocoderCNN(nn.Module):
    def __init__(self, mel_dim=80, output_length=16000):
        super(VocoderCNN, self).__init__()
        
        self.output_length = output_length  # Gi·∫£ s·ª≠ b·∫°n mu·ªën s√≥ng √¢m c√≥ 16000 samples

        # 1D Convolution layers for vocoder
        self.conv1 = nn.Conv1d(mel_dim, 256, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv1d(512, 1024, kernel_size=3, stride=1, padding=1)
        self.conv4 = nn.Conv1d(1024, 1, kernel_size=3, stride=1, padding=1)
        
        # Batch normalization layers
        self.bn1 = nn.BatchNorm1d(256)
        self.bn2 = nn.BatchNorm1d(512)
        self.bn3 = nn.BatchNorm1d(1024)

    def forward(self, mel_spec):
        x = F.relu(self.bn1(self.conv1(mel_spec)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        output_waveform = self.conv4(x)

        # ƒê·∫£m b·∫£o output_waveform c√≥ k√≠ch th∆∞·ªõc ƒë√∫ng
        output_waveform = output_waveform.squeeze(1)  # [batch_size, time_steps]
        if output_waveform.size(1) < self.output_length:
            output_waveform = F.pad(output_waveform, (0, self.output_length - output_waveform.size(1)))
        return output_waveform


# --- H√†m ph√°t √¢m thanh b·∫±ng PyAudio ---
def play_audio(waveform, sample_rate=16000):
    p = pyaudio.PyAudio()
    stream = p.open(format=pyaudio.paFloat32,
                    channels=1,
                    rate=sample_rate,
                    output=True)
    stream.write(waveform.astype(np.float32).tobytes())
    stream.stop_stream()
    stream.close()
    p.terminate()

# --- Kh·ªüi t·∫°o m√¥ h√¨nh ---
vocab_size = len(vocab)
speech_synthesizer = SpeechSynthesizer(vocab_size)
vocoder_cnn = VocoderCNN()

# --- Loss function v√† optimizer ---
loss_fn = nn.L1Loss()
optimizer = torch.optim.Adam(list(speech_synthesizer.parameters()) + list(vocoder_cnn.parameters()), lr=1e-3)

# --- Chu·∫©n b·ªã dataset --- (Gi·∫£ l·∫≠p)
dataset = [
    ("hello world", "hello.wav"),
    ("how are you", "how.wav"),
]

# --- Hu·∫•n luy·ªán m√¥ h√¨nh ---
print("üöÄ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh...\n")

for epoch in range(100):
    total_loss = 0
    for text, audio_path in dataset:
        if not os.path.exists(audio_path):
            print(f"‚ö†Ô∏è File {audio_path} kh√¥ng t·ªìn t·∫°i, b·ªè qua.")
            continue

        # Chu·∫©n b·ªã input
        text_seq = text_to_sequence(text, vocab)
        text_tensor = torch.tensor(text_seq, dtype=torch.long).unsqueeze(0)  # [batch_size, seq_len]

        mel_spec = audio_to_mel(audio_path)  # [1, mel_dim, time_steps]
        mel_spec = mel_spec.squeeze(0)  # Lo·∫°i b·ªè chi·ªÅu batch (n·∫øu c·∫ßn)
        mel_spec = mel_spec.transpose(0, 1)  # ƒê·∫£m b·∫£o c√°c chi·ªÅu kh·ªõp (mel_dim, time_steps)

        # Forward
        optimizer.zero_grad()

        mel_output = speech_synthesizer(text_tensor)  # [1, seq_len, mel_dim]
        mel_output_resized = mel_output.transpose(1, 2)  # [1, mel_dim, seq_len]

        # D·ª± ƒëo√°n waveform b·∫±ng vocoder CNN
        waveform_output = vocoder_cnn(mel_output_resized)  # [batch_size, time_steps]
        
        # Ki·ªÉm tra l·∫°i c√°c tensor tr∆∞·ªõc khi t√≠nh to√°n loss
        print(f"waveform_output size: {waveform_output.size()}")
        print(f"mel_spec size: {mel_spec.size()}")

        # ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng time steps c·ªßa waveform_output kh·ªõp v·ªõi mel_spec
        if waveform_output.size(1) != mel_spec.size(-1):  # mel_spec.size(-1) l·∫•y time_steps
            waveform_output_resized = F.interpolate(waveform_output.unsqueeze(0), size=mel_spec.size(-1), mode='linear', align_corners=False).squeeze(0)
        else:
            waveform_output_resized = waveform_output

        # Ki·ªÉm tra l·∫°i c√°c tensor sau khi ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc
        print(f"waveform_output_resized size: {waveform_output_resized.size()}")

        # T√≠nh loss
        loss = loss_fn(waveform_output_resized, mel_spec)  # [batch_size, time_steps]
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/100], Loss: {total_loss/len(dataset):.4f}")

print("\n‚úÖ Hu·∫•n luy·ªán xong!\n")

# --- L∆∞u m√¥ h√¨nh ---
torch.save(speech_synthesizer.state_dict(), 'speech_synthesizer.pth')
torch.save(vocoder_cnn.state_dict(), 'vocoder_cnn.pth')
print("üíæ ƒê√£ l∆∞u m√¥ h√¨nh")

# --- D·ª∞ ƒêO√ÅN: Nh·∫≠p vƒÉn b·∫£n v√† ph√°t √¢m thanh --- 
while True:
    text = input("üìù Nh·∫≠p vƒÉn b·∫£n: ")
    if text.lower() == 'exit':
        break

    if len(text.strip()) == 0:
        continue

    text_seq = text_to_sequence(text, vocab)
    text_tensor = torch.tensor(text_seq, dtype=torch.long).unsqueeze(0)

    with torch.no_grad():
        mel_output = speech_synthesizer(text_tensor)  # [1, seq_len, mel_dim]
        mel_output_resized = mel_output.transpose(1, 2)  # [1, mel_dim, seq_len]

        # D·ª± ƒëo√°n s√≥ng √¢m t·ª´ mel-spectrogram
        waveform = vocoder_cnn(mel_output_resized)  # [time_steps]

    # Ph√°t √¢m thanh
    print("üîä Ph√°t √¢m thanh...")
    play_audio(waveform.numpy(), sample_rate=16000)
    print("‚úÖ Xong!\n")
